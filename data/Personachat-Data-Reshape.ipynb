{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, shuffle, seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona Chat data strip out of personality, create persona tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# data : https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\n",
    "with open('./personachat_self_original.json') as f:\n",
    "    persona = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and valid\n",
    "train = persona['train']\n",
    "valid = persona['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17878"
      ]
     },
     "execution_count": 1422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 1423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data to have a list of all dialog\n",
    "full_data = train + valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only the personality in each dialogue\n",
    "personalities = [full_data[i]['personality'] for i in range(len(full_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes through all the personality descriptions to clean and sort\n",
    "for i in range(len(personalities)):\n",
    "    for k in range(len(personalities[i])):\n",
    "        personalities[i][k] = personalities[i][k].replace(\"'\", ' ').replace(' m ', ' am ').replace(' ve ', ' have ').replace(' ll ', ' will ')\n",
    "    personalities[i].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the personality in set so that we can easily find if a description of persona is a subset of another\n",
    "set_personalities = [set(personalities[i]) for i in range(len(full_data))]\n",
    "\n",
    "# get the personas that have 5 descriptions\n",
    "five_persona = [set_personalities[i] for i in range(len(set_personalities)) if len(set_personalities[i]) > 4]\n",
    "# get the personas that have 4 descriptions\n",
    "four_persona = [set_personalities[i] for i in range(len(set_personalities)) if len(set_personalities[i]) == 4]\n",
    "\n",
    "# finding the unique personas for 4 and 5 description list personas\n",
    "b_set = set(tuple(x) for x in five_persona)\n",
    "five_persona_unique = [ list(x) for x in b_set ]\n",
    "\n",
    "c_set = set(tuple(x) for x in four_persona)\n",
    "four_persona_unique = [ list(x) for x in c_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the description personas\n",
    "for four_lines in four_persona_unique:\n",
    "    four_lines.sort()\n",
    "    \n",
    "for five_lines in five_persona_unique:\n",
    "    five_lines.sort()\n",
    "\n",
    "# get the unique persona desciptions in sets rather than list\n",
    "five_persona_set_unique = [set(persona_set) for persona_set in five_persona_unique]\n",
    "\n",
    "four_persona_set_unique = [set(persona_set) for persona_set in four_persona_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4762"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(four_persona_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1350"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(five_persona_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes through all the personas and sees if it is a subset of a larger persona description\n",
    "# if it is it adds the index with the corresponding index and persona description len list in a tuple\n",
    "# this is to map the index to the correct larger persona description\n",
    "combinations = []\n",
    "for i in range(len(set_personalities)):\n",
    "    for k in range(len(five_persona_set_unique)):\n",
    "        if set_personalities[i].issubset(five_persona_set_unique[k]):\n",
    "            combinations.append((i,k,5))\n",
    "            break\n",
    "\n",
    "    if len(combinations) < i:\n",
    "        for j in range(len(four_persona_unique)):\n",
    "            if set_personalities[i].issubset(four_persona_set_unique[j]):\n",
    "                combinations.append((i,j,4))  \n",
    "                break\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18877"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18878"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(personalities)\n",
    "# missing one index in combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 420, 5), (21, 497, 5), (23, 1240, 5), (24, 361, 5), (25, 113, 5)]"
      ]
     },
     "execution_count": 1381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations[20:25]\n",
    "# skips 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# easy to find larger persona description from persona tag number \n",
    "persona_dict_numbers = {}\n",
    "for i in range(len(five_persona_unique)):\n",
    "    persona_dict_numbers[i] = five_persona_unique[i]\n",
    "for k in range(len(four_persona_unique)):\n",
    "    persona_dict_numbers[k+len(five_persona_unique)] = four_persona_unique[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1436,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am married .',\n",
       " 'i love to read .',\n",
       " 'my favorite place is the beach .',\n",
       " 'my husband is a cop .',\n",
       " 'teaching is my passion .']"
      ]
     },
     "execution_count": 1436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of consolidated persona description\n",
    "# <persona15>\n",
    "persona_dict_numbers.get(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to loop through to find tag personas\n",
    "index_df = pd.DataFrame(data = combinations, columns = ['i_persona', 'i_unique', 'n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df.loc[22,'i_persona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# go through each dialog and find which persona tag and larger persona description it is\n",
    "# create new lists and add these in it to have the lareger persona descriptions / persona tags in the right index\n",
    "k = 0\n",
    "bigger_persona = []\n",
    "bigger_persona_named = []\n",
    "for i in range(len(personalities)):\n",
    "    # checks if the indexes are the same / same data point\n",
    "    if i==index_df.loc[k, 'i_persona']:\n",
    "        # get the index of the unique persona, waiting to see if it is 4 or 5 persona description\n",
    "        index = index_df.loc[k, 'i_unique']\n",
    "        # if the unique index corresponds to the 5 persona description, add the specific 5 persona into the list\n",
    "        if index_df.loc[k, 'n'] == 5:\n",
    "            bigger_persona.append(five_persona_unique[index])\n",
    "            bigger_persona_named.append(['<persona' + str(index)+ '>'])\n",
    "            \n",
    "        # if the unique index corresponds to the 4 persona description, add the specific 4 persona into the list\n",
    "        elif index_df.loc[k, 'n'] == 4:\n",
    "            bigger_persona.append(four_persona_unique[index])\n",
    "            bigger_persona_named.append(['<persona' + str(index+len(five_persona_unique))+ '>'])\n",
    "\n",
    "        k+=1\n",
    "    # there is one personality that we hard code\n",
    "    else:\n",
    "        bigger_persona.append(personalities[i])\n",
    "        # hard code this persona\n",
    "        hard_code_number = len(five_persona_unique) + len(four_persona_unique)\n",
    "        persona_dict_numbers[hard_code_number] = personalities[i]\n",
    "        bigger_persona_named.append(['<persona'+ str(hard_code_number) + '>'])\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_persona_train_val(full_data, train_len, personality_list_replacement):\n",
    "    \"\"\"replace the personality in the full data with the new consolidated personality description/tags\n",
    "        and break it to train and validation\n",
    "        returns a dictionary keys train and valid\"\"\"\n",
    "    data = copy.deepcopy(full_data)\n",
    "    for i in range(len(full_data)):\n",
    "        # replace personality\n",
    "        data[i]['personality'] = personality_list_replacement[i]\n",
    "        \n",
    "    train_data = data[:train_len]\n",
    "    valid_data = data[train_len:]\n",
    "\n",
    "    full_data_changed = {'train': train_data,\n",
    "                  'valid': valid_data}\n",
    "    \n",
    "    return full_data_changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_named = split_persona_train_val(full_data, len(train), bigger_persona_named)\n",
    "full_data_bigger = split_persona_train_val(full_data, len(train), bigger_persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('personachat_data_named_bigger.json', 'w') as outfile:\n",
    "#     json.dump(full_data_named, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('personachat_data_bigger.json', 'w') as outfile:\n",
    "#     json.dump(full_data_bigger, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample persona chat - training can't run the full data - takes too long or memory runs out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with all the personas\n",
    "persona_more = pd.DataFrame(data = bigger_persona_named, columns = ['persona'])\n",
    "persona_more['count'] = 1\n",
    "\n",
    "# group the personas to see how many times it appears in the dataset\n",
    "count_personas = persona_more.groupby('persona').sum().reset_index()\n",
    "\n",
    "# get a list of the personas that appear more than 4 times in this dataset\n",
    "more_than_four_dialog_persona = count_personas[count_personas['count'] > 4]['persona'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train_val(persona_tags_to_sample, larger_data_persona_tags, larger_data, num_sample, val_size, random_state):\n",
    "    \"\"\" samples a larger persona data\n",
    "        returns a dictionary with train and valid keys\"\"\"\n",
    "    \n",
    "    seed(random_state)\n",
    "    # samples a number of persona tags\n",
    "    sample_personas = sample(persona_tags_to_sample, num_sample)\n",
    "    \n",
    "    # larger_data_persona_tags = [larger_data[i]['personality'] for i in range(len(larger_data))]\n",
    "    \n",
    "    # finds the indices of the larger_data_persoa_tags to see where in the larger_data these personas are at\n",
    "    # larger_data_persona_tags and larger_data should have the same persona tags in each indices\n",
    "    # this is so that we can subset the larger_data that includes these personas\n",
    "    sample_persona_indices = []\n",
    "    for persona_i in sample_personas:\n",
    "        indices = [i for i, x in enumerate(larger_data_persona_tags) if x[0] == persona_i]\n",
    "        sample_persona_indices.extend(indices)\n",
    "    sample_persona_indices.sort()\n",
    "    \n",
    "    #subset of data\n",
    "    sample_data = [larger_data[i] for i in sample_persona_indices]\n",
    "    \n",
    "    #get personality of the subset of data\n",
    "    sample_data_persona = [data_i['personality'] for data_i in sample_data]\n",
    "    \n",
    "    # balanced by personalitiy\n",
    "    train_sample, valid_sample = train_test_split(sample_data, test_size = val_size, random_state= random_state, stratify = sample_data_persona)\n",
    "    \n",
    "    train_valid_sample = {'train': train_sample,\n",
    "               'valid': valid_sample}\n",
    "    \n",
    "    # get the unique persona tags and creates a py file that has a list of the unique persona tags\n",
    "    with open('./unique_persona_tag_'+str(num_sample)+'.py', mode='w') as file:\n",
    "        file.write(\"persona_tags = ['\" +  \"', '\".join(sample_personas) + \"']\")\n",
    "    file.close()\n",
    "    \n",
    "    return train_valid_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_combined = full_data_named['train'] + full_data_named['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples 100 unique personas\n",
    "train_valid_100 = sample_train_val(persona_tags_to_sample = more_than_four_dialog_persona, \n",
    "                 larger_data_persona_tags = bigger_persona_named, \n",
    "                 larger_data = full_data_combined, \n",
    "                 num_sample = 100,\n",
    "                 val_size = 0.2,\n",
    "                 random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines the train and valid and then replaces the persona tags with descriptions\n",
    "num_sample_combined = train_valid_num['train'] + train_valid_num['valid']\n",
    "# gets the persona descriptions from the persona tags\n",
    "num_sample_combined_list_order = [persona_dict_numbers.get(\n",
    "    int(re.sub('\\D','' ,persona_['personality'][0]))) for persona_ in num_sample_combined]\n",
    "\n",
    "# gets the dataset that uses the persona descriptions\n",
    "train_valid_num_list = split_persona_train_val(num_sample_combined, \n",
    "                                               len(train_valid_num['train']), \n",
    "                                               num_sample_combined_list_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_100.json', 'w') as outfile:\n",
    "#     json.dump(train_valid_num_list, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open('sample_persona_named_100.json', 'w') as outfile:\n",
    "#     json.dump(train_valid_num, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 4list where for personality it is a list of persona tags with the last tag as the correct persona tag\n",
    "### similar concept as guessing for candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_persona_named_list = [data_i[0] for data_i in bigger_persona_named]\n",
    "unique_personas = list(np.unique(bigger_persona_named))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_4list(persona_tag_list):\n",
    "    \"\"\"from the list of persona tags, create the same length list where each element is\n",
    "    a list of four persona tags with the last one being the correct persona\"\"\"\n",
    "    unique_persona_tags = list(np.unique(persona_tag_list))\n",
    "    new_4list = []\n",
    "    for persona_tag_i in persona_tag_list:\n",
    "        sample_persona_list = sample(unique_persona_tags, 3)\n",
    "        # make sure that the sample_persona_list doesn't have the correct persona\n",
    "        while persona_tag_i in sample_persona_list:\n",
    "            # take out the correct persona\n",
    "            sample_persona_list.remove(persona_tag_i)\n",
    "            \n",
    "            # add a new persona\n",
    "            new_add = sample(unique_persona_tags, 1)\n",
    "            sample_persona_list.extend(new_add)\n",
    "        # add the correct persona tag at the last index (3)\n",
    "        sample_persona_list.append(persona_tag_i)\n",
    "        \n",
    "        new_4list.append(sample_persona_list)\n",
    "        \n",
    "    return new_4list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the personality tags of the num_sample\n",
    "num_sample_combined_split = [data_i['personality'] for data_i in num_sample_combined]\n",
    "num_sample_combined_split = [data_i['personality'][0] for data_i in num_sample_combined]\n",
    "num_personality4_last = create_4list(num_sample_combined_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_num_4_last = split_persona_train_val(num_sample_combined, \n",
    "                                                 len(train_valid_num['train']), \n",
    "                                                 num_personality4_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_4named_500.json', 'w') as outfile:\n",
    "#     json.dump(train_valid_num_4_last, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('personachat_data_named_bigger_4last.json', 'w') as outfile:\n",
    "#     json.dump(full_data_named_4last, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_10 = sample_train_val(persona_tags_to_sample = more_than_four_dialog_persona, \n",
    "                 larger_data_persona_tags = bigger_persona_named, \n",
    "                 larger_data = full_data_combined, \n",
    "                 num_sample = 10,\n",
    "                 val_size = 0.3,\n",
    "                 random_state = 334322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 1143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_valid_10['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_10 = train_valid_10['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sample_persona = [valid_10[i]['personality'] for i in range(len(valid_10))]\n",
    "valid_more, test_more = train_test_split(valid_10, test_size = 0.5, random_state= 13, stratify = valid_sample_persona)\n",
    "\n",
    "train_valid_more = {'train': train_valid_10['train'],\n",
    "               'valid': valid_more}\n",
    "\n",
    "test_more_dict = {'train': train_valid_10['train'],\n",
    "                  'valid':test_more}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_named_10_train_val.json', 'w') as outfile:\n",
    "#     json.dump(train_valid_more, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_combined = train_valid_more['train'] + train_valid_more['valid']\n",
    "test_more_combined = test_more_dict['train'] + test_more_dict['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_combined_persona = [data_i['personality'][0] for data_i in train_valid_combined]\n",
    "test_combined_persona = [data_i['personality'][0] for data_i in test_more_combined]\n",
    "\n",
    "train_valid_personality4_last = create_4list(train_valid_combined_persona)\n",
    "test_personality4_last = create_4list(test_combined_persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_persona_list_10 = [persona_dict_numbers.get(int(re.sub('\\D','' ,persona_['personality'][0]))) for persona_ in train_valid_combined]\n",
    "test_persona_list_10 = [persona_dict_numbers.get(int(re.sub('\\D','' ,persona_['personality'][0]))) for persona_ in test_more_combined]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10_list = split_persona_train_val(train_valid_combined, \n",
    "                                                 len(train_valid_more['train']), \n",
    "                                                 train_valid_persona_list_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10_4named = split_persona_train_val(train_valid_combined, \n",
    "                                                 len(train_valid_more['train']), \n",
    "                                                 train_valid_personality4_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_10_list = split_persona_train_val(test_more_dict, \n",
    "                                                 len(test_more_dict['train']), \n",
    "                                                 test_persona_list_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_10_4named = split_persona_train_val(test_more_dict, \n",
    "                                                 len(test_more_dict['train']), \n",
    "                                                 test_personality4_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_10_train_val.json', 'w') as outfile:\n",
    "#     json.dump(train_10_list, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_4named_100_train_val.json', 'w') as outfile:\n",
    "#     json.dump(train_valid_more_4_last, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sample4_last_dialog = copy.deepcopy(more_sample_combined)\n",
    "for i in range(len(more_sample4_last_dialog)):\n",
    "    convo_last = more_sample4_last_dialog[i]['utterances'][-1]\n",
    "    dialog = convo_last['history'] + [convo_last['candidates'][-1]]\n",
    "    more_sample4_last_dialog[i]['utterances'] = dialog\n",
    "train_more_4last_dialog = more_sample4_last_dialog[:len(train_more)]\n",
    "valid_more_4last_dialog = more_sample4_last_dialog[len(train_more):]\n",
    "\n",
    "train_valid_more_4_last_dialog = {'train': train_more_4last_dialog,\n",
    "               'valid': valid_more_4last_dialog}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_4named_100_train_val_dialog.json', 'w') as outfile:\n",
    "#     json.dump(train_valid_more_4_last_dialog, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_4named_100_test.json', 'w') as outfile:\n",
    "#     json.dump(test_set, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sample4_last_dialog_test = copy.deepcopy(more_sample4_last_test)\n",
    "for i in range(len(more_sample4_last_dialog_test)):\n",
    "    convo_last = more_sample4_last_dialog_test[i]['utterances'][-1]\n",
    "    dialog = convo_last['history'] + [convo_last['candidates'][-1]]\n",
    "    more_sample4_last_dialog_test[i]['utterances'] = dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_4named_100_test_dialog.json', 'w') as outfile:\n",
    "#     json.dump(more_sample4_last_dialog_test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./unique_persona_tag_100_dialog.py', mode='w') as file:\n",
    "#     file.write(\"persona_tags = ['\" +  \"', '\".join(sample_personas) + \"']\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  create data sets for human evaluation\n",
    "### version 1: see 4 shuffled personality descriptions with a conversations and match the correct personality with the given dialog\n",
    "### version 2: see 4 conversation with their persona tag, the 5th conversation has a persona of one of the previous 4 persona tags, guess the correct persona tag with the given dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# with open('sample_persona_4named_100_train_val.json') as f:\n",
    "#     human_eval = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# with open('sample_persona_4named_100_train_val.json') as f:\n",
    "#     human_eval_dialog = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# to create the human evaluation data\n",
    "with open('sample_persona_named_100_train_val.json') as f:\n",
    "    human_eval_dialog = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the first 80 dialog in train - need 20 * 4 dialog\n",
    "# need 4 different dialog with different personas and have a last persona that\n",
    "# is unknown to the user but show the dialog and classify which persona out of the\n",
    "# previous 4 personas spoke in this dialog\n",
    "human_eval_80 = human_eval_dialog['train'][:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab validation data, this will be used for the task\n",
    "# to guess the persona by the shuffled persona descriptions (the correct persona is not necessarily \n",
    "# the last description) [human evaluation set version 1]\n",
    "# to guess the persona given\n",
    "# previous 4 dialogs with persona tags [human evaluation set version 2]\n",
    "human_eval_valid = human_eval_dialog['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# to create the human evaluation data\n",
    "with open('sample_persona_4named_100_train_val.json') as f:\n",
    "    human_eval_4dialog = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_4valid = human_eval_4dialog['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# take the first 20 dialog in the validation\n",
    "# shuffle the persona tags in the personality\n",
    "# map the persona tags with their description\n",
    "# grab only the full conversation rather than all the candidates and history\n",
    "# make it easier for the user to see the conversation and the personality lists\n",
    "human_eval_20 = human_eval_4valid[:20]\n",
    "more_sample4_last_dialog_test = copy.deepcopy(human_eval_20)\n",
    "for i in range(len(more_sample4_last_dialog_test)):\n",
    "    convo_last = more_sample4_last_dialog_test[i]['utterances'][-1]\n",
    "    dialog = convo_last['history'] + [convo_last['candidates'][-1]]\n",
    "    more_sample4_last_dialog_test[i]['utterances'] = dialog\n",
    "    personas = more_sample4_last_dialog_test[i]['personality']\n",
    "    shuffle(personas)\n",
    "    more_sample4_last_dialog_test[i]['personality'] = [persona_dict_numbers.get(int(re.sub('\\D','' ,persona_))) for persona_ in personas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the personas in valid data set\n",
    "# using this data to have the masked persona, so that the user can test which\n",
    "# persona is speaking\n",
    "valid_persona = [human_eval_valid[i]['personality'] for i in range(len(human_eval_valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the personas in the 80 dialog to easily extract the personas by index for sampling later\n",
    "personality_80 = [human_eval_80[i]['personality'] for i in range(80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the utterances just the dialog (last history with last candidate of last utterance) \n",
    "# between the persona and the other speaker\n",
    "# easier for the user to see the dialog rather than have the candidates and history\n",
    "human_valid = copy.deepcopy(human_eval_valid)\n",
    "for i in range(len(human_valid)):\n",
    "    convo_last = human_valid[i]['utterances'][-1]\n",
    "    dialog = convo_last['history'] + [convo_last['candidates'][-1]]\n",
    "    human_valid[i]['utterances'] = dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the training data with just the dialog as well\n",
    "# make it easier for the user to read the conversation\n",
    "human_80 = copy.deepcopy(human_eval_80)\n",
    "for i in range(len(human_80)):\n",
    "    convo_last = human_80[i]['utterances'][-1]\n",
    "    dialog = convo_last['history'] + [convo_last['candidates'][-1]]\n",
    "    human_80[i]['utterances'] = dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "# create the human evaluation data\n",
    "# classifying 20 dialogs\n",
    "# get 4 conversations \n",
    "# sample the 4 personas for 1 persona to have the masked persona\n",
    "# find one of the conversations with that 1 persona is in and add it after the 4 personas\n",
    "# replace the persona tag with strings for that conversation\n",
    "human_eval_copy = copy.deepcopy(human_valid)\n",
    "human_eval_4 = []\n",
    "answer_key_valid = []\n",
    "for i in range(20):\n",
    "    human_eval_4.extend(human_80[i*4:(i+1)*4])\n",
    "    four_persona = personality_80[i*4:(i+1)*4]\n",
    "    sample_persona = sample(four_persona,1)[0]\n",
    "    index = valid_persona.index(sample_persona)\n",
    "    sample_valid = copy.deepcopy(human_eval_copy[index])\n",
    "    answer_key_valid.append(sample_valid['personality'])\n",
    "    sample_valid['personality'] = ''\n",
    "    \n",
    "    human_eval_4.append(sample_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_100_human_eval_dialog_20_v2_answer_key.json', 'w') as outfile:\n",
    "#     json.dump(answer_key_valid, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sample_persona_100_human_eval_dialog_20.json', 'w') as outfile:\n",
    "#     json.dump(more_sample4_last_dialog_test, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
